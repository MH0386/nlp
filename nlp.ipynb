{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get URL and Get HTML from it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "response = requests.get(URL, timeout=5)\n",
    "HTML = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(HTML, \"html.parser\")\n",
    "paragraph = soup.find_all(\"p\")\n",
    "TEXT = [paragraph[i].text for i in range(len(paragraph))]\n",
    "TEXT = \" \".join(TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = re.sub(r\"[^a-zA-Z0-9]\", \" \", TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = re.sub(r\"\\s+\", \" \", TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Lower Case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = TEXT.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = word_tokenize(TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = [word for word in TEXT if word not in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words with Length Less than 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_LESS_3 = [word for word in TEXT if len(word) < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Words with Length Less than 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = [word for word in TEXT if len(word) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "TEXT = [ps.stem(word) for word in TEXT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "TEXT = [lemmatizer.lemmatize(word) for word in TEXT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = list(set(TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Unique Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1940',\n",
       " '1980',\n",
       " '1990',\n",
       " '2003',\n",
       " '2010',\n",
       " '2015',\n",
       " '2020',\n",
       " '2023',\n",
       " 'abil',\n",
       " 'accur',\n",
       " 'achiev',\n",
       " 'acl',\n",
       " 'acquir',\n",
       " 'act',\n",
       " 'action',\n",
       " 'address',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'age',\n",
       " 'aid',\n",
       " 'alan',\n",
       " 'algorithm',\n",
       " 'align',\n",
       " 'along',\n",
       " 'alreadi',\n",
       " 'although',\n",
       " 'among',\n",
       " 'analyz',\n",
       " 'announc',\n",
       " 'answer',\n",
       " 'anymor',\n",
       " 'appar',\n",
       " 'appli',\n",
       " 'applic',\n",
       " 'approach',\n",
       " 'area',\n",
       " 'art',\n",
       " 'articl',\n",
       " 'articul',\n",
       " 'artifici',\n",
       " 'aspect',\n",
       " 'author',\n",
       " 'autom',\n",
       " 'base',\n",
       " 'becam',\n",
       " 'becom',\n",
       " 'behaviour',\n",
       " 'bengio',\n",
       " 'best',\n",
       " 'branch',\n",
       " 'british',\n",
       " 'brno',\n",
       " 'broadli',\n",
       " 'build',\n",
       " 'call',\n",
       " 'capabl',\n",
       " 'captur',\n",
       " 'care',\n",
       " 'categor',\n",
       " 'categori',\n",
       " 'caus',\n",
       " 'challeng',\n",
       " 'chine',\n",
       " 'chomskyan',\n",
       " 'close',\n",
       " 'cluster',\n",
       " 'coars',\n",
       " 'code',\n",
       " 'cognit',\n",
       " 'collect',\n",
       " 'colleg',\n",
       " 'combin',\n",
       " 'commonli',\n",
       " 'complex',\n",
       " 'comprehens',\n",
       " 'comput',\n",
       " 'concern',\n",
       " 'confer',\n",
       " 'confront',\n",
       " 'conll',\n",
       " 'construct',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'context',\n",
       " 'contextu',\n",
       " 'conveni',\n",
       " 'corpu',\n",
       " 'corpus',\n",
       " 'coupl',\n",
       " 'cpu',\n",
       " 'criterion',\n",
       " 'data',\n",
       " 'dataset',\n",
       " 'decis',\n",
       " 'deep',\n",
       " 'defin',\n",
       " 'depend',\n",
       " 'develop',\n",
       " 'development',\n",
       " 'devi',\n",
       " 'dictionari',\n",
       " 'direct',\n",
       " 'discourag',\n",
       " 'divis',\n",
       " 'document',\n",
       " 'domin',\n",
       " 'drawback',\n",
       " 'due',\n",
       " 'earliest',\n",
       " 'either',\n",
       " 'elabor',\n",
       " 'electron',\n",
       " 'embed',\n",
       " 'emul',\n",
       " 'end',\n",
       " 'energi',\n",
       " 'engin',\n",
       " 'especi',\n",
       " 'exampl',\n",
       " 'experi',\n",
       " 'explain',\n",
       " 'explicit',\n",
       " 'extract',\n",
       " 'extrapol',\n",
       " 'featur',\n",
       " 'field',\n",
       " 'find',\n",
       " 'first',\n",
       " 'flurri',\n",
       " 'follow',\n",
       " 'framework',\n",
       " 'free',\n",
       " 'frequent',\n",
       " 'friston',\n",
       " 'function',\n",
       " 'futur',\n",
       " 'gener',\n",
       " 'georg',\n",
       " 'give',\n",
       " 'given',\n",
       " 'goal',\n",
       " 'gradual',\n",
       " 'gram',\n",
       " 'grammar',\n",
       " 'hand',\n",
       " 'hard',\n",
       " 'health',\n",
       " 'healthcar',\n",
       " 'help',\n",
       " 'heritag',\n",
       " 'heurist',\n",
       " 'hidden',\n",
       " 'higher',\n",
       " 'histor',\n",
       " 'howev',\n",
       " 'human',\n",
       " 'idea',\n",
       " 'import',\n",
       " 'improv',\n",
       " 'inaccess',\n",
       " 'includ',\n",
       " 'increas',\n",
       " 'increasingli',\n",
       " 'ineffici',\n",
       " 'inform',\n",
       " 'inher',\n",
       " 'insight',\n",
       " 'intellig',\n",
       " 'interdisciplinari',\n",
       " 'intermedi',\n",
       " 'interpret',\n",
       " 'intertwin',\n",
       " 'introduct',\n",
       " 'invent',\n",
       " 'involv',\n",
       " 'john',\n",
       " 'karl',\n",
       " 'knowledg',\n",
       " 'lakoff',\n",
       " 'languag',\n",
       " 'larg',\n",
       " 'larger',\n",
       " 'late',\n",
       " 'law',\n",
       " 'layer',\n",
       " 'le',\n",
       " 'learn',\n",
       " 'length',\n",
       " 'lessen',\n",
       " 'level',\n",
       " 'likewis',\n",
       " 'limit',\n",
       " 'linguist',\n",
       " 'list',\n",
       " 'llm',\n",
       " 'london',\n",
       " 'long',\n",
       " 'lookup',\n",
       " 'machin',\n",
       " 'machineri',\n",
       " 'made',\n",
       " 'mainstream',\n",
       " 'maintain',\n",
       " 'major',\n",
       " 'mani',\n",
       " 'manipul',\n",
       " 'markov',\n",
       " 'match',\n",
       " 'measur',\n",
       " 'medicin',\n",
       " 'mental',\n",
       " 'method',\n",
       " 'methodolog',\n",
       " 'mid',\n",
       " 'mikolov',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'model',\n",
       " 'moor',\n",
       " 'mostli',\n",
       " 'multi',\n",
       " 'multimod',\n",
       " 'natur',\n",
       " 'necessari',\n",
       " 'need',\n",
       " 'network',\n",
       " 'neural',\n",
       " 'neurosci',\n",
       " 'neuroscientist',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'newli',\n",
       " 'nlp',\n",
       " 'note',\n",
       " 'notion',\n",
       " 'nuanc',\n",
       " 'observ',\n",
       " 'obsolet',\n",
       " 'offer',\n",
       " 'old',\n",
       " 'one',\n",
       " 'operation',\n",
       " 'operationaliz',\n",
       " 'organ',\n",
       " 'other',\n",
       " 'otherwis',\n",
       " 'overperform',\n",
       " 'par',\n",
       " 'part',\n",
       " 'particular',\n",
       " 'partli',\n",
       " 'patient',\n",
       " 'perceptron',\n",
       " 'period',\n",
       " 'perspect',\n",
       " 'phd',\n",
       " 'phrasebook',\n",
       " 'popular',\n",
       " 'possibl',\n",
       " 'power',\n",
       " 'premis',\n",
       " 'presenc',\n",
       " 'previous',\n",
       " 'primarili',\n",
       " 'principl',\n",
       " 'privaci',\n",
       " 'probabilist',\n",
       " 'problem',\n",
       " 'process',\n",
       " 'produc',\n",
       " 'properti',\n",
       " 'propos',\n",
       " 'protect',\n",
       " 'psycholinguist',\n",
       " 'psycholog',\n",
       " 'publish',\n",
       " 'pursu',\n",
       " 'question',\n",
       " 'rare',\n",
       " 'real',\n",
       " 'recent',\n",
       " 'recognit',\n",
       " 'record',\n",
       " 'recurr',\n",
       " 'refer',\n",
       " 'replac',\n",
       " 'repres',\n",
       " 'represent',\n",
       " 'requir',\n",
       " 'research',\n",
       " 'result',\n",
       " 'reviv',\n",
       " 'revolut',\n",
       " 'room',\n",
       " 'root',\n",
       " 'rule',\n",
       " 'scienc',\n",
       " 'scientif',\n",
       " 'searl',\n",
       " 'see',\n",
       " 'seek',\n",
       " 'semant',\n",
       " 'sen',\n",
       " 'separ',\n",
       " 'sequenc',\n",
       " 'seri',\n",
       " 'serv',\n",
       " 'set',\n",
       " 'sever',\n",
       " 'share',\n",
       " 'show',\n",
       " 'similar',\n",
       " 'simpl',\n",
       " 'sinc',\n",
       " 'singl',\n",
       " 'solv',\n",
       " 'sort',\n",
       " 'speak',\n",
       " 'specif',\n",
       " 'speech',\n",
       " 'stand',\n",
       " 'start',\n",
       " 'state',\n",
       " 'statist',\n",
       " 'steadi',\n",
       " 'stem',\n",
       " 'step',\n",
       " 'still',\n",
       " 'strong',\n",
       " 'student',\n",
       " 'studi',\n",
       " 'style',\n",
       " 'subdivid',\n",
       " 'subfield',\n",
       " 'subtask',\n",
       " 'summar',\n",
       " 'support',\n",
       " 'symbol',\n",
       " 'system',\n",
       " 'tag',\n",
       " 'task',\n",
       " 'technic',\n",
       " 'techniqu',\n",
       " 'technolog',\n",
       " 'test',\n",
       " 'text',\n",
       " 'theoret',\n",
       " 'theoretician',\n",
       " 'theori',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'tie',\n",
       " 'time',\n",
       " 'titl',\n",
       " 'tom',\n",
       " 'tool',\n",
       " 'topic',\n",
       " 'toward',\n",
       " 'train',\n",
       " 'trajectori',\n",
       " 'transform',\n",
       " 'translat',\n",
       " 'tree',\n",
       " 'trend',\n",
       " 'ture',\n",
       " 'turn',\n",
       " 'two',\n",
       " 'underli',\n",
       " 'underpin',\n",
       " 'understand',\n",
       " 'univers',\n",
       " 'uptak',\n",
       " 'use',\n",
       " 'variou',\n",
       " 'well',\n",
       " 'went',\n",
       " 'whose',\n",
       " 'widespread',\n",
       " 'winter',\n",
       " 'within',\n",
       " 'word',\n",
       " 'word2vec',\n",
       " 'world',\n",
       " 'would',\n",
       " 'write',\n",
       " 'written',\n",
       " 'year',\n",
       " 'yoshua']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Words Less Than 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '47',\n",
       " '48',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '8',\n",
       " '9',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'ai',\n",
       " 'co',\n",
       " 'co',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'e',\n",
       " 'g',\n",
       " 'g',\n",
       " 'g',\n",
       " 'g',\n",
       " 'g',\n",
       " 'g',\n",
       " 'g',\n",
       " 'j',\n",
       " 'n',\n",
       " 'r']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(TEXT_LESS_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
